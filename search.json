[
  {
    "objectID": "index.html#prÃ©-requisitos",
    "href": "index.html#prÃ©-requisitos",
    "title": "Do Zero Ã  RegressÃ£o Linear: com Dois Sabores",
    "section": "ğŸ§© PrÃ©-requisitos",
    "text": "ğŸ§© PrÃ©-requisitos\nO curso foi desenhado para ser acessÃ­vel a quem quer aprender RegressÃ£o Linear de verdade - da base matemÃ¡tica Ã  aplicaÃ§Ã£o prÃ¡tica, mesmo sem formaÃ§Ã£o formal em EstatÃ­stica ou Engenharia.\n\nğŸ“ Conhecimentos desejÃ¡veis (nÃ£o obrigatÃ³rios)\n\nMatemÃ¡tica bÃ¡sica do ensino mÃ©dio: operaÃ§Ãµes com fraÃ§Ãµes, potÃªncias e equaÃ§Ãµes lineares simples.\n\nNoÃ§Ãµes intuitivas de funÃ§Ãµes e grÃ¡ficos: o que significa uma reta, inclinaÃ§Ã£o, intercepto, plano x-y e espaÃ§o x-y-z.\n\nLeitura em inglÃªs tÃ©cnico: nomes de variÃ¡veis, termos como dataset, model, gradient, etc.\n\nCuriosidade com programaÃ§Ã£o: Python para dados serve como ponto de partida, mas nÃ£o Ã© necessÃ¡rio."
  },
  {
    "objectID": "index.html#objetivo-do-curso",
    "href": "index.html#objetivo-do-curso",
    "title": "Do Zero Ã  RegressÃ£o Linear: com Dois Sabores",
    "section": "ğŸ¯ Objetivo do Curso",
    "text": "ğŸ¯ Objetivo do Curso\nDar uma formaÃ§Ã£o sÃ³lida em MatemÃ¡tica, Modelagem EstatÃ­stica (Probabilidade e InferÃªncia) e Machine Learning (PrediÃ§Ã£o), tendo a RegressÃ£o Linear como fio condutor.\nO aluno vai aprender sobre:\n\nO mundo da inferÃªncia (probabilidade, estatÃ­stica clÃ¡ssica, inferÃªncia e estimaÃ§Ã£o, teorema do limite central, p-valor, teste de hipÃ³teses, intervalos de confianÃ§a, etc.);\nO mundo da prediÃ§Ã£o (contexto de ML supervisionado, gradiente descendente, erro empÃ­rico, bias-variance);\nE que ambos partem da mesma base: Algebra linear + Probabilidade + OtimizaÃ§Ã£o.\n\nNÃ£o Ã© apenas um â€œCurso de RegressÃ£o Linearâ€, mas sim um curso completo da intuiÃ§Ã£o dos fundamentos matemÃ¡ticos por trÃ¡s de cada passo."
  },
  {
    "objectID": "index.html#estrutura-didÃ¡tica",
    "href": "index.html#estrutura-didÃ¡tica",
    "title": "Do Zero Ã  RegressÃ£o Linear: com Dois Sabores",
    "section": "ğŸš€ Estrutura DidÃ¡tica ",
    "text": "ğŸš€ Estrutura DidÃ¡tica \n\n\n\nMÃ³dulo\nTema\nÃŠnfase\n\n\n\n\n0\nO que Ã© modelar dados\nIntuiÃ§Ã£o e motivaÃ§Ã£o\n\n\n1\nFundamentos MatemÃ¡ticos\nÃlgebra Linear + OtimizaÃ§Ã£o\n\n\n2\nFundamentos EstatÃ­sticos\nProbabilidade + InferÃªncia\n\n\n3\nRegressÃ£o Linear (EstatÃ­stica)\nOLS e interpretaÃ§Ã£o\n\n\n4\nRegressÃ£o Linear (ML)\nGradient Descent e prediÃ§Ã£o"
  },
  {
    "objectID": "index.html#conteÃºdo-completo",
    "href": "index.html#conteÃºdo-completo",
    "title": "Do Zero Ã  RegressÃ£o Linear: com Dois Sabores",
    "section": "ğŸ“˜ ConteÃºdo Completo",
    "text": "ğŸ“˜ ConteÃºdo Completo\nCada pilar Ã© construÃ­do com foco em intuiÃ§Ã£o, formalizaÃ§Ã£o e prÃ¡tica.\n\nÃlgebra Linear\n\nBase geomÃ©trica da regressÃ£o.\n\n\nVetores, matrizes e operaÃ§Ãµes elementares\n\nProduto interno e projeÃ§Ãµes ortogonais\n\nSubespaÃ§o de colunas e interpretaÃ§Ã£o geomÃ©trica de \\(X\\beta\\)\nSoluÃ§Ã£o matricial da regressÃ£o linear:\n\\(\\hat{\\beta} = (X'X)^{-1}X'Y\\)\n\nInterpretaÃ§Ã£o de resÃ­duos como vetores ortogonais\n\nRepresentaÃ§Ã£o grÃ¡fica em 2D e 3D\n\n\n\nProbabilidade ClÃ¡ssica\n\nA linguagem da incerteza.\n\n\nVariÃ¡veis aleatÃ³rias e distribuiÃ§Ãµes\n\nEsperanÃ§a, variÃ¢ncia e covariÃ¢ncia\n\nLei dos Grandes NÃºmeros e intuiÃ§Ã£o da mÃ©dia amostral\n\nIntroduÃ§Ã£o ao conceito de ruÃ­do: \\(\\varepsilon_i \\sim (0, \\sigma^2)\\)\n\nConstruÃ§Ã£o intuitiva do modelo probabilÃ­stico:\n\\(Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\\)\n\n\n\nInferÃªncia EstatÃ­stica\n\nComo estimar, testar e interpretar.\n\n\nOLS como melhor estimador linear nÃ£o viesado (Gaussâ€“Markov)\n\nErro padrÃ£o, intervalo de confianÃ§a e teste t\n\nTeste F e interpretaÃ§Ã£o global do modelo\n\nHipÃ³teses e p-valores (sem decorar fÃ³rmulas)\n\nDiagnÃ³stico de resÃ­duos e suposiÃ§Ãµes do modelo\n\nIdeia: Causalidade vs.Â CorrelaÃ§Ã£o â€” quando o \\(\\beta\\) Ã© causal?\n\n\n\nOtimizaÃ§Ã£o\n\nO motor por trÃ¡s da prediÃ§Ã£o.\n\n\nRevisÃ£o de derivadas e gradiente vetorial\n\nFunÃ§Ãµes convexas e mÃ­nimos locais\n\nDerivada matricial da soma dos quadrados\n\nGradient Descent: passo de atualizaÃ§Ã£o e intuiÃ§Ã£o geomÃ©trica\n\nFunÃ§Ã£o de perda (Loss Function) e erro empÃ­rico\n\nIntroduÃ§Ã£o Ã  regularizaÃ§Ã£o: Ridge e LASSO\n\nConvergÃªncia, taxa de aprendizado e visualizaÃ§Ã£o do gradiente"
  },
  {
    "objectID": "index.html#por-que-dois-sabores",
    "href": "index.html#por-que-dois-sabores",
    "title": "Do Zero Ã  RegressÃ£o Linear: com Dois Sabores",
    "section": "ğŸ’¡ Por que â€œdois saboresâ€?",
    "text": "ğŸ’¡ Por que â€œdois saboresâ€?\nPorque vocÃª vai aprender a mesma regressÃ£o linear sob dois paradigmas:\nğŸ¦ EstatÃ­stico (inferÃªncia e causalidade) e\nğŸ¨ Computacional (prediÃ§Ã£o e otimizaÃ§Ã£o)."
  },
  {
    "objectID": "index.html#formato",
    "href": "index.html#formato",
    "title": "Do Zero Ã  RegressÃ£o Linear: com Dois Sabores",
    "section": "ğŸ§± Formato",
    "text": "ğŸ§± Formato\n\nVÃ­deos curtos e progressivos, lanÃ§ados semanalmente\n\nExercÃ­cios prÃ¡ticos em Python\n\nVisualizaÃ§Ãµes geomÃ©tricas e intuiÃ§Ãµes claras\n\nComunidade e feedback (para turmas pagas)"
  },
  {
    "objectID": "index.html#inscreva-se",
    "href": "index.html#inscreva-se",
    "title": "Do Zero Ã  RegressÃ£o Linear: com Dois Sabores",
    "section": "ğŸ“ Inscreva-se ",
    "text": "ğŸ“ Inscreva-se \nQuer ser avisado quando abrirem as inscriÃ§Ãµes?\n\nCarregandoâ€¦\n\n\nğŸ”” Deixe seu contato no formulÃ¡rio acima e receba o convite para a primeira turma com preÃ§o promocional de lanÃ§amento.\n\n\n\n\n\n\nCaio Velasco Ã© Engenheiro MecÃ¢nico Cum Laude pela UFRJ e Mestre em Economia e PolÃ­ticas PÃºblicas pela UCLA, com bolsa da FundaÃ§Ã£o Lemann. Atualmente mora em Barcelona, na Espanha.\nDepois de pausar um doutorado em Economia na Holanda durante a pandemia, passou a atuar remotamente em Projetos de Engenharia e CiÃªncia de Dados, ajudando empresas dos EUA, Europa e Brasil a construÃ­rem plataformas modernas e preparar modelos para responder perguntas de negÃ³cio. PorÃ©m, a paixÃ£o por ensinar Ã© o que mais o motiva.\nAntes disso, trabalhou com FinTech e EdTech e fundou o MePrepara, plataforma online que ensinou matemÃ¡tica para as provas GRE e GMAT.\nReconhecido por instituiÃ§Ãµes como UCLA, Yale, FundaÃ§Ã£o Lemann, General Electric Foundation e Club of Rome na SuiÃ§a, Caio combina engenharia, economia e educaÃ§Ã£o - e acredita que entender a matemÃ¡tica por trÃ¡s dos modelos sem perder a intuiÃ§Ã£o e a aplicaÃ§Ã£o prÃ¡tica Ã© essencial.\n\nğŸ’¡ â€œMeu objetivo Ã© ensinar com clareza e propÃ³sito - conectando teoria, intuiÃ§Ã£o e aplicaÃ§Ã£o prÃ¡tica.â€"
  }
]