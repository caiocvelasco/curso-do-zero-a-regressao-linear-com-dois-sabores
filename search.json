[
  {
    "objectID": "index.html#minha-motivaÃ§Ã£o",
    "href": "index.html#minha-motivaÃ§Ã£o",
    "title": "Do Zero Ã  RegressÃ£o Linear - com Dois Sabores",
    "section": "Minha MotivaÃ§Ã£o ",
    "text": "Minha MotivaÃ§Ã£o \nEm um vÃ­deo de um fÃ­sico americano muito famoso, Richard Feynman diz que toda explicaÃ§Ã£o depende de um quadro de referÃªncia compartilhado entre quem pergunta e quem responde.\nAntes de responder por que algo acontece, Ã© preciso concordar sobre o que jÃ¡ sabemos ser verdade.\nÃ‰ aÃ­ que mora o perigo. A maioria das pessoas ainda nÃ£o estÃ¡ confortÃ¡vel com (e ainda nÃ£o domina) a formalizaÃ§Ã£o matemÃ¡tica e estatÃ­stica que estÃ¡ atrÃ¡s de muito fenÃ´meno, inclusive no mundo dos dados. Portanto, Ã© nesse gap que entra a minha filosofia de ensino. Eu quero te convencer que:\n\nVocÃª Ã© capaz sim de entender coisas complexas, mas precisa de alguÃ©m capaz de traduzi-las para o seu quadro de referÃªncia atual\nVocÃª Ã© capaz sim de expandir seu quadro de referÃªncia atual, para poder ter espaÃ§o para colocar as verdadeiras respostas das suas possÃ­veis perguntas.\n\nÃ‰ exatamente assim com a RegressÃ£o Linear â€” e com a CiÃªncia de Dados em geral.\nQuando alguÃ©m pergunta â€œpor que o modelo funciona?â€, a resposta exige atravessar vÃ¡rias camadas: Ã¡lgebra linear, probabilidade, inferÃªncia, otimizaÃ§Ã£o.\n\nSe vocÃª nÃ£o conhece essas bases, a explicaÃ§Ã£o soa misteriosa ou com certeza eu estarei trapaceando para te responder;\n\nSe vocÃª conhece, ela se torna Ã³bvia, elegante e te empodera no longo prazo.\n\nEste curso nasce dessa motivaÃ§Ã£o:\n\nCriar um caminho que reconstrÃ³i o quadro conceitual para que a matemÃ¡tica e estatÃ­stica passem a fazer sentido.\n\n\nâ€œEntender Ã© compartilhar o mesmo vocabulÃ¡rio da explicaÃ§Ã£o e pressupÃµe que conseguimos atingir o mesmo quadro de referÃªncia.â€"
  },
  {
    "objectID": "index.html#exemplo",
    "href": "index.html#exemplo",
    "title": "Do Zero Ã  RegressÃ£o Linear - com Dois Sabores",
    "section": "Exemplo",
    "text": "Exemplo\nAluno:\n- â€œCaio, por que a RegressÃ£o Linear funciona? Por que a gente usa isso?â€\nEu (sorrindo):\n- â€œDepende do que vocÃª quer dizer comâ€funcionaâ€. - Se vocÃª quer dizer â€˜por que a reta se ajusta aos pontosâ€™, entÃ£o estamos falando de Ã¡lgebra linear e geometria. - Se vocÃª quer dizer â€˜por que ela estima a relaÃ§Ã£o verdadeira entre Y e Xâ€™, entramos em probabilidade e inferÃªncia. - E se vocÃª quer dizer â€˜por que ela prevÃª bem novos dadosâ€™, aÃ­ jÃ¡ estamos em otimizaÃ§Ã£o e machine learning.\nEntÃ£oâ€¦ a resposta depende do quadro em que vocÃª estÃ¡!\nE Ã© exatamente isso que eu quero construir neste curso: te nivelar por cima para vocÃª atingir o quadro conceitual onde tudo isso se conecta."
  },
  {
    "objectID": "index.html#objetivo-do-curso",
    "href": "index.html#objetivo-do-curso",
    "title": "Do Zero Ã  RegressÃ£o Linear - com Dois Sabores",
    "section": "ğŸ¯ Objetivo do Curso ",
    "text": "ğŸ¯ Objetivo do Curso \nDar uma formaÃ§Ã£o sÃ³lida em MatemÃ¡tica, EstatÃ­stica e Machine Learning,\ntendo a RegressÃ£o Linear como fio condutor para unir teoria, geometria e prÃ¡tica.\nVocÃª vai compreender:\n\nO mundo da EstatÃ­stica (inferÃªncia) - como estimamos, testamos e interpretamos relaÃ§Ãµes entre variÃ¡veis.\n\nO mundo do Machine Learning (prediÃ§Ã£o e otimizaÃ§Ã£o) - como treinamos modelos para prever e generalizar.\n\nE que ambos partem da mesma base: Ã¡lgebra linear + probabilidade + otimizaÃ§Ã£o."
  },
  {
    "objectID": "index.html#por-que-este-curso-Ã©-diferente-e-por-que-dois-sabores",
    "href": "index.html#por-que-este-curso-Ã©-diferente-e-por-que-dois-sabores",
    "title": "Do Zero Ã  RegressÃ£o Linear - com Dois Sabores",
    "section": "ğŸ¤” Por que este curso Ã© diferente e Por que â€œdois saboresâ€?",
    "text": "ğŸ¤” Por que este curso Ã© diferente e Por que â€œdois saboresâ€?\nA maioria dos cursos ensina regressÃ£o como uma receita.\nAqui, vocÃª vai entender o porquÃª de cada passo - e ver como a matemÃ¡tica, quando bem explicada, se torna clara, visual e atÃ© bonita.\n\nğŸ‘ï¸ Visual: vocÃª vai enxergar o que as equaÃ§Ãµes representam no espaÃ§o.\n\nğŸ”¢ MatemÃ¡tico: aprender o essencial de Ã¡lgebra, probabilidade e otimizaÃ§Ã£o no contexto certo.\n\nğŸ’¡ Aplicado: ver a teoria funcionando em Python e em dados reais.\n\nğŸ¯ Conectado: entender como EstatÃ­stica e Machine Learning sÃ£o duas formas de olhar o mesmo problema.\n\nAlÃ©m disso, vocÃª vai aprender a mesma RegressÃ£o Linear sob dois paradigmas:\n- EstatÃ­stico (inferÃªncia e causalidade) â€” entender por que e como as variÃ¡veis se relacionam.\n- Computacional (prediÃ§Ã£o e otimizaÃ§Ã£o) â€” treinar modelos que funcionam bem em novos dados.\nAlgum desses dois modos de pensar aparecem em praticamente todas as indÃºstrias:\n\nğŸ¥ EducaÃ§Ã£o, SaÃºde e Epidemiologia: regressÃ£o Ã© usada para estimar o efeito de medicamentos, identificar fatores de risco e ajustar por variÃ¡veis de confusÃ£o - foco na inferÃªncia causal.\n\nğŸ›’ E-commerce e Marketing: usada para prever vendas, calcular propensÃ£o Ã  compra ou estimar o impacto de campanhas - foco na prediÃ§Ã£o e otimizaÃ§Ã£o.\n\nâš™ï¸ Engenharia e IoT: modelos lineares ajudam a prever falhas, calibrar sensores e otimizar desempenho de mÃ¡quinas - combinando modelagem estatÃ­stica e preditiva.\n\nEm todos esses contextos, a matemÃ¡tica Ã© a mesma - o que muda Ã© o propÃ³sito: explicar ou prever."
  },
  {
    "objectID": "index.html#prÃ©-requisitos",
    "href": "index.html#prÃ©-requisitos",
    "title": "Do Zero Ã  RegressÃ£o Linear - com Dois Sabores",
    "section": "ğŸ§© PrÃ©-requisitos ",
    "text": "ğŸ§© PrÃ©-requisitos \nEste curso foi feito para quem quer aprender RegressÃ£o Linear de verdade - da base Ã  aplicaÃ§Ã£o prÃ¡tica, mesmo sem formaÃ§Ã£o formal em EstatÃ­stica ou Engenharia.\n\nğŸ“ Conhecimentos desejÃ¡veis\n\nMatemÃ¡tica bÃ¡sica do ensino mÃ©dio: fraÃ§Ãµes, potÃªncias e equaÃ§Ãµes lineares simples.\n\nFunÃ§Ãµes e grÃ¡ficos: o que significa uma reta, inclinaÃ§Ã£o, intercepto e plano \\(x\\)-\\(y\\).\n\nLeitura em inglÃªs tÃ©cnico: termos como dataset, model, gradient.\n\nCuriosidade com programaÃ§Ã£o: Python Ã© bem-vindo, mas nÃ£o obrigatÃ³rio."
  },
  {
    "objectID": "index.html#conteÃºdo-completo",
    "href": "index.html#conteÃºdo-completo",
    "title": "Do Zero Ã  RegressÃ£o Linear - com Dois Sabores",
    "section": "ğŸ“˜ ConteÃºdo Completo ",
    "text": "ğŸ“˜ ConteÃºdo Completo \nO curso foi desenhado para que cada conceito matemÃ¡tico surja como uma resposta natural a uma pergunta prÃ¡tica.\nVocÃª nÃ£o vai decorar fÃ³rmulas: vai entender por que elas existem, o que representam e como se conectam.\n\nEstrutura Geral\n\n\n\nMÃ³dulo\nTema\nÃŠnfase\n\n\n\n\n0\nO que Ã© modelar dados\nIntuiÃ§Ã£o e motivaÃ§Ã£o\n\n\n1\nFundamentos MatemÃ¡ticos\nÃlgebra Linear + OtimizaÃ§Ã£o\n\n\n2\nFundamentos EstatÃ­sticos\nProbabilidade + InferÃªncia\n\n\n3\nRegressÃ£o Linear (EstatÃ­stica)\nOLS e interpretaÃ§Ã£o\n\n\n4\nRegressÃ£o Linear (ML)\nGradient Descent e prediÃ§Ã£o\n\n\n\n\n\nÃlgebra Linear: a casa da regressÃ£o linear\n-&gt; Como uma equaÃ§Ã£o vira um modelo? Onde a regressÃ£o realmente â€œviveâ€?\n\nCada coluna do dataset (\\(X_1, X_2, ..., X_p\\)) Ã© uma variÃ¡vel explicativa, uma direÃ§Ã£o em um espaÃ§o vetorial. O conjunto dessas colunas forma o subespaÃ§o de colunas de \\(X\\) - Ã© dentro dele que o modelo tenta â€œreconstruirâ€ \\(Y\\), a variÃ¡vel que queremos explicar.\nO dataset como matriz: um conjunto de observaÃ§Ãµes pode ser escrito como uma matriz \\(X\\) de tamanho \\(n \\times p\\), onde cada linha representa uma observaÃ§Ã£o e cada coluna, uma variÃ¡vel. Ã‰ aqui que o dataset ganha forma geomÃ©trica: um objeto que podemos manipular, projetar e decompor.\nEspaÃ§o e subespaÃ§o: representam todas as combinaÃ§Ãµes possÃ­veis das variÃ¡veis. Mostram atÃ© onde o modelo pode ir - o que ele consegue e o que jamais explicarÃ¡.\nBase e dimensÃ£o: indicam quantas variÃ¡veis realmente trazem informaÃ§Ã£o nova, e quando algumas apenas repetem o que outras jÃ¡ dizem (multicolinearidade).\nProjeÃ§Ã£o: ajustar a regressÃ£o Ã© projetar \\(Y\\) sobre o espaÃ§o gerado por \\(X\\) - encontrar a melhor combinaÃ§Ã£o linear das colunas de \\(X\\) que aproxima \\(Y\\).\nOrtogonalidade e resÃ­duos: os resÃ­duos sÃ£o perpendiculares a esse espaÃ§o; isso significa que o modelo jÃ¡ extraiu tudo o que \\(X\\) podia oferecer.\nSoluÃ§Ã£o matricial: \\(\\hat{\\beta} = (X'X)^{-1}X'Y\\) expressa exatamente o processo de projeÃ§Ã£o de \\(Y\\) sobre o espaÃ§o das colunas de \\(X\\). A fÃ³rmula mostra como o modelo â€œcombinaâ€ as colunas de \\(X\\) para construir a versÃ£o de \\(Y\\) mais prÃ³xima possÃ­vel - \\(\\hat{Y} = X\\hat{\\beta}\\).\n\n\nA regressÃ£o linear Ã©, geometricamente, a melhor projeÃ§Ã£o possÃ­vel de \\(Y\\) no espaÃ§o definido por \\(X\\).\n\n\n\n\nProbabilidade ClÃ¡ssica - a casa da incerteza presente nos dados\n\nAntes, \\(Y\\) e \\(X\\) eram vetores no espaÃ§o. Agora, eles passam a ser variÃ¡veis aleatÃ³rias (random variables) - porque agora a necessidade Ã© entender o que Ã© incerteza, amostra, etc.\n\nA partir daqui, tratamos o modelo como uma crenÃ§a formal sobre como os dados sÃ£o gerados: um Data Generating Process (DGP) ou um â€œmecanismo gerador dos dadosâ€. Ã‰ o elo entre a realidade e o modelo: o que assumimos ser verdadeiro sobre o processo que produz as observaÃ§Ãµes, ou seja, os dados.\n\nO DGP como mecanismo:\nSupomos que existe uma relaÃ§Ã£o verdadeira na populaÃ§Ã£o que explica como X se relaciona com Y, e que nossa amostra Ã© apenas uma manifestaÃ§Ã£o aleatÃ³ria desse processo. Ã‰ aqui que nasce a noÃ§Ã£o de incerteza: e de que â€œmodeloâ€ significa assumir algo sobre o mundo.\nVariÃ¡veis AleatÃ³rias (random variables):\nAs colunas de \\(X\\) e o vetor \\(Y\\) na Ãlgebra Linear agora sÃ£o variÃ¡veis aleatÃ³rias, ou seja, objetos que assumem valores diferentes a cada extraÃ§Ã£o do DGP, como uma certa probabilidade atribuÃ­da a cada valor. Tipo quando jogamos uma moeda e cada valor tem chance 50% de aparecer. Esse conceito transforma o dataset \\((Y, X)\\) abstrato em uma realizaÃ§Ã£o de algo maior: uma amostra aleatÃ³ria.\nTipos de variÃ¡veis:\nO tipo de variÃ¡vel define o espaÃ§o onde ela vive - um conceito que vem da MatemÃ¡tica mais refinada:\n\nQuando o conjunto de valores possÃ­veis Ã© contÃ¡vel - finito (como \\({0,1}\\)) ou infinito mas enumerÃ¡vel (como os naturais \\({1,2,3,...}\\)) - dizemos que a variÃ¡vel Ã© discreta.\nExemplo: nÃºmero de filhos, quantidade de produtos vendidos.\n\nQuando o conjunto de valores Ã© infinito e nÃ£o enumerÃ¡vel, dizemos que a variÃ¡vel Ã© contÃ­nua. Exemplo: altura, renda, temperatura.\n\nEssa diferenÃ§a muda completamente a forma da distribuiÃ§Ã£o - e, portanto, do modelo:\n\nSe \\(Y\\) Ã© contÃ­nua â†’ usamos RegressÃ£o Linear.\n\nSe \\(Y\\) Ã© binÃ¡ria (0/1) â†’ RegressÃ£o LogÃ­stica.\n\nSe alguma variÃ¡vel \\(X\\) Ã© categÃ³rica â†’ representamos por dummies (1 para presenÃ§a, 0 para ausÃªncia), criando novas dimensÃµes no espaÃ§o vetorial.\n\nğŸ’¡ Nota:\nUma outra confusÃ£o vem do fato de que nem toda variÃ¡vel discreta Ã© â€œcategÃ³ricaâ€. Uma variÃ¡vel pode ser discreta com ou sem ordem entre os valores:\n\nOrdinal (hÃ¡ ordem, como nÃ­vel de satisfaÃ§Ã£o: â€œbaixoâ€, â€œmÃ©dioâ€, â€œaltoâ€);\n\nNominal (sem ordem, como â€œsexoâ€ ou â€œcor dos olhosâ€).\n\n\nEntender a natureza da variÃ¡vel evita erros conceituais - e garante que cada tipo seja modelado de forma coerente com sua estrutura matemÃ¡tica.\n\nDistribuiÃ§Ã£o e incerteza:\nCada variÃ¡vel aleatÃ³ria tem uma distribuiÃ§Ã£o, que descreve a probabilidade de cada valor possÃ­vel. A distribuiÃ§Ã£o Ã© a forma matemÃ¡tica de expressar a incerteza e o alicerce de todo raciocÃ­nio estatÃ­stico.\nAmostra aleatÃ³ria:\nAs observaÃ§Ãµes que temos sÃ£o uma amostra independente do DGP. Isso legitima usar os dados observados para aprender sobre a populaÃ§Ã£o.\nModelo probabilÃ­stico:\nEspecificamos a crenÃ§a de que:\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\cdots + \\beta_p X_{ip} + \\varepsilon_i,\\]\nonde \\(\\varepsilon_i\\) representa o componente aleatÃ³rio, com \\(\\mathbb{E}[\\varepsilon_i] = 0\\) e \\(\\text{Var}(\\varepsilon_i) = \\sigma^2\\).\n\nO modelo agora Ã© mais do que uma reta - Ã© uma afirmaÃ§Ã£o probabilÃ­stica sobre o comportamento de \\(Y\\) dado \\(X\\).\n\nModelo estatÃ­stico:\nÃ‰ o conjunto de todos os possÃ­veis DGPs compatÃ­veis com as suposiÃ§Ãµes (linearidade, homocedasticidade, independÃªncia, normalidade dos erros). Isso define os limites do que podemos inferir. Quando essas suposiÃ§Ãµes quebram, o modelo deixa de representar o mundo que imaginamos.\n\n\n\nğŸ¯ A probabilidade dÃ¡ Ã  regressÃ£o um novo significado:\nnÃ£o mais â€œqual reta cabe melhor nos pontosâ€, mas\nâ€œqual mecanismo plausÃ­vel pode ter gerado esses pontosâ€.\n\n\n\nInferÃªncia EstatÃ­stica\n\nOLS como melhor estimador linear nÃ£o viesado (Gaussâ€“Markov)\n\nErro padrÃ£o, intervalo de confianÃ§a e teste t\n\nTeste F e interpretaÃ§Ã£o global do modelo\n\nHipÃ³teses e p-valores\n\nDiagnÃ³stico de resÃ­duos e suposiÃ§Ãµes do modelo\n\nIdeia de Causalidade vs.Â CorrelaÃ§Ã£o - quando o \\(\\beta\\) Ã© causal?\n\n\n\nOtimizaÃ§Ã£o\n\nRevisÃ£o de derivadas e gradiente vetorial\n\nFunÃ§Ãµes convexas e mÃ­nimos locais\n\nDerivada matricial da soma dos quadrados\n\nGradient Descent: passo de atualizaÃ§Ã£o e intuiÃ§Ã£o geomÃ©trica\n\nFunÃ§Ã£o de perda (Loss Function) e erro empÃ­rico\n\nIntroduÃ§Ã£o Ã  regularizaÃ§Ã£o: Ridge e LASSO\n\nConvergÃªncia, taxa de aprendizado e visualizaÃ§Ã£o do gradiente"
  },
  {
    "objectID": "index.html#formato",
    "href": "index.html#formato",
    "title": "Do Zero Ã  RegressÃ£o Linear - com Dois Sabores",
    "section": "ğŸ§± Formato",
    "text": "ğŸ§± Formato\n\nVÃ­deos curtos e progressivos, lanÃ§ados semanalmente\n\nExercÃ­cios prÃ¡ticos em Python\n\nVisualizaÃ§Ãµes geomÃ©tricas e intuiÃ§Ãµes claras\n\nComunidade e feedback"
  },
  {
    "objectID": "index.html#inscreva-se",
    "href": "index.html#inscreva-se",
    "title": "Do Zero Ã  RegressÃ£o Linear - com Dois Sabores",
    "section": "ğŸ“ Inscreva-se ",
    "text": "ğŸ“ Inscreva-se \nQuer ser avisado quando abrirem as inscriÃ§Ãµes?\n\nCarregandoâ€¦\n\n\n\n\n\n\nCaio Velasco Ã© Engenheiro MecÃ¢nico Cum Laude pela UFRJ e Mestre em Economia e PolÃ­ticas PÃºblicas pela UCLA, com bolsa da FundaÃ§Ã£o Lemann. Atualmente mora em Barcelona, na Espanha.\nDepois de pausar um doutorado em Economia na Holanda durante a pandemia, passou a atuar remotamente em Projetos de Engenharia e CiÃªncia de Dados, ajudando empresas dos EUA, Europa e Brasil a construÃ­rem plataformas modernas e preparar modelos para responder perguntas de negÃ³cio. PorÃ©m, a paixÃ£o por ensinar Ã© o que mais o motiva.\nAntes disso, trabalhou com FinTech e EdTech e fundou o MePrepara, plataforma online que ensinou matemÃ¡tica para as provas GRE e GMAT.\nReconhecido por instituiÃ§Ãµes como UCLA, Yale, FundaÃ§Ã£o Lemann, General Electric Foundation e Club of Rome na SuiÃ§a, Caio combina engenharia, economia e educaÃ§Ã£o - e acredita que entender a matemÃ¡tica por trÃ¡s dos modelos sem perder a intuiÃ§Ã£o e a aplicaÃ§Ã£o prÃ¡tica Ã© essencial.\n\nğŸ’¡ â€œMeu objetivo Ã© ensinar com clareza e propÃ³sito - conectando teoria, intuiÃ§Ã£o e aplicaÃ§Ã£o prÃ¡tica.â€"
  }
]