---
title: "Do Zero à Regressão Linear - com Dois Sabores"
format: html
---

<a name="topo"></a>

> Eu não quero vender um curso. **Quero vender uma forma de pensar**.

## Minha Motivação <a name="motivacao"></a>

Em um vídeo de um físico americano muito famoso, _Richard Feynman_ diz que **toda explicação depende de um quadro de referência compartilhado entre quem pergunta e quem responde**.  
Antes de responder *por que* algo acontece, é preciso **concordar sobre o que já sabemos ser verdade**.

É aí que mora o perigo. A maioria das pessoas ainda não está confortável com (e ainda não domina) a formalização matemática e estatística que está atrás de muito fenômeno, inclusive no mundo dos **dados**. Portanto, é nesse _gap_ que entra a minha **filosofia de ensino**. Eu quero te convencer que:

- **Você é capaz sim de entender coisas complexas**, mas precisa de alguém capaz de traduzi-las para o seu quadro de referência atual
- **Você é capaz sim de expandir seu quadro de referência atual**, para poder ter espaço para colocar as verdadeiras respostas das suas possíveis perguntas. 

É exatamente assim com a Regressão Linear — e com a Ciência de Dados em geral.

Quando alguém pergunta *“por que o modelo funciona?”*, a resposta exige atravessar várias camadas: _álgebra linear, probabilidade, inferência, otimização_.  

- Se você não conhece essas bases, a explicação soa misteriosa ou com certeza eu estarei trapaceando para te responder;  
- Se você conhece, ela se torna óbvia, elegante e te empodera no longo prazo.

Este curso nasce dessa motivação:  

- **Criar um caminho que reconstrói o quadro conceitual** para que a matemática e estatística passem a fazer sentido.

> “Entender é compartilhar o mesmo vocabulário da explicação e pressupõe que conseguimos atingir o mesmo quadro de referência.”

## Exemplo

**Aluno**:  
- “Caio, por que a Regressão Linear funciona? Por que a gente usa isso?”

**Eu (sorrindo)**:  
- “Depende do que você quer dizer com "funciona".
- Se você quer dizer ‘_por que a reta se ajusta aos pontos_’, então estamos falando de **álgebra linear** e **geometria**.
- Se você quer dizer ‘_por que ela estima a relação verdadeira entre Y e X_’, entramos em **probabilidade** e **inferência**.
- E se você quer dizer ‘_por que ela prevê bem novos dados_’, aí já estamos em **otimização** e **machine learning**.

Então... a resposta depende do quadro em que você está!

E é exatamente isso que eu quero construir neste curso: te nivelar por cima para você atingir o quadro conceitual onde tudo isso se conecta.

## 🎯 Objetivo do Curso <a name="objetivo"></a>

Dar uma formação sólida em **Matemática, Estatística e Machine Learning**,  
tendo a **Regressão Linear** como fio condutor para unir teoria, geometria e prática.

Você vai compreender:

- O *mundo da Estatística (inferência)* - como estimamos, testamos e interpretamos relações entre variáveis.  
- O *mundo do Machine Learning (predição e otimização)* - como treinamos modelos para prever e generalizar.  
- E que ambos partem da mesma base: **álgebra linear + probabilidade + otimização**.

## 🤔 Por que este curso é diferente e Por que “dois sabores”?

A maioria dos cursos ensina regressão como uma receita.  
Aqui, você vai **entender o porquê** de cada passo - e ver como a matemática, quando bem explicada, se torna **clara, visual e até bonita**.

- 👁️ **Visual:** você vai enxergar o que as equações representam no espaço.  
- 🔢 **Matemático:** aprender o essencial de álgebra, probabilidade e otimização no contexto certo.  
- 💡 **Aplicado:** ver a teoria funcionando em Python e em dados reais.  
- 🎯 **Conectado:** entender como Estatística e Machine Learning são duas formas de olhar o mesmo problema.

Além disso, você vai aprender **a mesma Regressão Linear** sob dois paradigmas:  
- **Estatístico (inferência e causalidade)** — entender *por que* e *como* as variáveis se relacionam.  
- **Computacional (predição e otimização)** — treinar modelos que funcionam bem em novos dados.

Algum desses dois modos de pensar aparecem em praticamente todas as indústrias:

- 🏥 **Educação, Saúde e Epidemiologia:** regressão é usada para estimar o efeito de medicamentos, identificar fatores de risco e ajustar por variáveis de confusão - foco na **inferência causal**.  
- 🛒 **E-commerce e Marketing:** usada para prever vendas, calcular propensão à compra ou estimar o impacto de campanhas - foco na **predição e otimização**.  
- ⚙️ **Engenharia e IoT:** modelos lineares ajudam a prever falhas, calibrar sensores e otimizar desempenho de máquinas - combinando **modelagem estatística e preditiva**.

Em todos esses contextos, a matemática é a mesma - o que muda é **o propósito**: explicar ou prever.

## 🧩 Pré-requisitos <a name="requisitos"></a>

Este curso foi feito para quem **quer aprender Regressão Linear de verdade - da base à aplicação prática**,
mesmo sem formação formal em Estatística ou Engenharia.

### 🎓 Conhecimentos desejáveis
- **Matemática básica do ensino médio:** frações, potências e equações lineares simples.  
- **Funções e gráficos:** o que significa uma reta, inclinação, intercepto e plano $x$-$y$.  
- **Leitura em inglês técnico:** termos como *dataset*, *model*, *gradient*.  
- **Curiosidade com programação:** Python é bem-vindo, mas não obrigatório.

## 📘 Conteúdo Completo <a name="modulos"></a>

O curso foi desenhado para que cada conceito matemático surja como uma resposta natural a uma pergunta prática.  
Você não vai decorar fórmulas: vai **entender por que elas existem**, o que representam e como se conectam.

### Estrutura Geral

| Módulo | Tema | Ênfase |
|--------|------|--------|
| 0 | O que é modelar dados | Intuição e motivação |
| 1 | Fundamentos Matemáticos | Álgebra Linear + Otimização |
| 2 | Fundamentos Estatísticos | Probabilidade + Inferência |
| 3 | Regressão Linear (Estatística) | OLS e interpretação |
| 4 | Regressão Linear (ML) | Gradient Descent e predição |

### Álgebra Linear: a casa da regressão linear

-> _Como uma equação vira um modelo? Onde a regressão realmente “vive”?_

- Cada coluna do dataset ($X_1, X_2, ..., X_p$) é uma **variável explicativa**, uma direção em um espaço vetorial.
O conjunto dessas colunas forma o **subespaço de colunas de $X$** - é dentro dele que o modelo tenta “reconstruir” $Y$, a variável que queremos explicar.
- **O dataset como matriz**: um conjunto de observações pode ser escrito como uma **matriz $X$ de tamanho $n \times p$**, onde cada linha representa uma observação e cada coluna, uma variável. É aqui que o dataset ganha **forma geométrica**: um objeto que podemos manipular, projetar e decompor.
- **Espaço e subespaço**: representam **todas as combinações possíveis das variáveis**. Mostram até onde o modelo pode ir - o que ele consegue e o que jamais explicará.
- **Base e dimensão**: indicam **quantas variáveis realmente trazem informação nova**, e quando algumas apenas repetem o que outras já dizem (**multicolinearidade**).
- **Projeção**: ajustar a regressão é **projetar $Y$ sobre o espaço gerado por $X$** - encontrar a melhor combinação linear das colunas de $X$ que aproxima $Y$.
- **Ortogonalidade e resíduos**: os resíduos são **perpendiculares** a esse espaço; isso significa que o modelo **já extraiu tudo o que $X$ podia oferecer**.

- **Solução matricial**: $\hat{\beta} = (X'X)^{-1}X'Y$ expressa exatamente o processo de projeção de $Y$ sobre o espaço das colunas de $X$. A fórmula mostra como o modelo “combina” as colunas de $X$ para construir a versão de $Y$ mais próxima possível - $\hat{Y} = X\hat{\beta}$.

> A regressão linear é, geometricamente, **a melhor projeção possível** de $Y$ no espaço definido por $X$.

---

### Probabilidade Clássica - a casa da incerteza presente nos dados

> Antes, $Y$ e $X$ eram vetores no espaço. Agora, eles passam a ser variáveis aleatórias (_random variables_) - porque agora a necessidade é entender o que é incerteza, amostra, etc.

A partir daqui, tratamos o modelo como uma crença formal sobre como os dados são gerados: um _Data Generating Process (DGP)_ ou um "mecanismo gerador dos dados". É o elo entre a realidade e o modelo: o que assumimos ser verdadeiro sobre o processo que produz as observações, ou seja, os dados.

- **O DGP como mecanismo:**  
  Supomos que existe uma relação verdadeira na população que explica como X se relaciona com Y, e que nossa amostra é apenas uma manifestação aleatória desse processo. É aqui que nasce a noção de incerteza: e de que “modelo” significa *assumir algo sobre o mundo.*

- **Variáveis Aleatórias (random variables):**  
  As colunas de $X$ e o vetor $Y$ na Álgebra Linear agora são **variáveis aleatórias**, ou seja, objetos que assumem valores diferentes a cada extração do DGP, como uma certa probabilidade atribuída a cada valor. Tipo quando jogamos uma moeda e cada valor tem chance 50% de aparecer. Esse conceito transforma o dataset $(Y, X)$ abstrato em uma **realização** de algo maior: uma **amostra aleatória**.

- **Tipos de variáveis:**  
  O tipo de variável define o **espaço onde ela vive** - um conceito que vem da Matemática mais refinada:

  - Quando o conjunto de valores possíveis é **contável** - finito (como ${0,1}$) ou infinito mas enumerável (como os naturais ${1,2,3,...}$) - dizemos que a variável é **discreta**.  
    Exemplo: número de filhos, quantidade de produtos vendidos.  
  - Quando o conjunto de valores é **infinito e não enumerável**, dizemos que a variável é **contínua**. Exemplo: altura, renda, temperatura.

  Essa diferença muda completamente a forma da distribuição - e, portanto, do modelo:  
  - Se $Y$ é contínua → usamos **Regressão Linear**.  
  - Se $Y$ é binária (0/1) → **Regressão Logística**.  
  - Se alguma variável $X$ é categórica → representamos por **dummies** (1 para presença, 0 para ausência), criando novas dimensões no espaço vetorial.

  💡 *Nota:*  
  Uma outra confusão vem do fato de que **nem toda variável discreta é “categórica”**. Uma variável pode ser discreta **com ou sem ordem** entre os valores:  
  - **Ordinal** (há ordem, como nível de satisfação: “baixo”, “médio”, “alto”);  
  - **Nominal** (sem ordem, como “sexo” ou “cor dos olhos”).  

Entender a natureza da variável evita erros conceituais - e garante que cada tipo seja modelado de forma coerente com sua estrutura matemática.

- **Distribuição e incerteza:**  
  Cada variável aleatória tem uma **distribuição**, que descreve a probabilidade de cada valor possível. A distribuição é a forma matemática de expressar a incerteza e o alicerce de todo raciocínio estatístico.

- **Amostra aleatória:**  
  As observações que temos são uma **amostra independente do DGP**. Isso legitima usar os dados observados para aprender sobre a população.

- **Modelo probabilístico:**  
  Especificamos a crença de que:  
  $$Y_i = \beta_0 + \beta_1 X_{i1} + \cdots + \beta_p X_{ip} + \varepsilon_i,$$  
  onde $\varepsilon_i$ representa o componente aleatório, com $\mathbb{E}[\varepsilon_i] = 0$ e $\text{Var}(\varepsilon_i) = \sigma^2$.  
 
 O modelo agora é mais do que uma reta - é uma **afirmação probabilística** sobre o comportamento de $Y$ dado $X$.

- **Modelo estatístico:**  
  É o conjunto de todos os possíveis DGPs compatíveis com as suposições (linearidade, homocedasticidade, independência, normalidade dos erros). Isso define **os limites do que podemos inferir**. Quando essas suposições quebram, o modelo deixa de representar o mundo que imaginamos.

---

> 🎯 A probabilidade dá à regressão um novo significado:  
> não mais “qual reta cabe melhor nos pontos”, mas  
> “qual mecanismo plausível pode ter gerado esses pontos”.

### Inferência Estatística

- OLS como **melhor estimador linear não viesado (Gauss–Markov)**  
- Erro padrão, intervalo de confiança e teste t  
- Teste F e interpretação global do modelo  
- Hipóteses e p-valores  
- Diagnóstico de resíduos e suposições do modelo  
- Ideia de Causalidade vs. Correlação - quando o $\beta$ é causal?

### Otimização

- Revisão de derivadas e gradiente vetorial  
- Funções convexas e mínimos locais  
- Derivada matricial da soma dos quadrados  
- Gradient Descent: passo de atualização e intuição geométrica  
- Função de perda (Loss Function) e erro empírico  
- Introdução à regularização: Ridge e LASSO  
- Convergência, taxa de aprendizado e visualização do gradiente

## 🧱 Formato

- **Vídeos curtos e progressivos**, lançados semanalmente  
- **Exercícios práticos em Python**  
- **Visualizações geométricas e intuições claras**  
- **Comunidade e feedback**

## 📝 Inscreva-se <a name="formulario"></a>

Quer ser avisado quando abrirem as inscrições?

<iframe src="https://docs.google.com/forms/d/e/1FAIpQLScGXyserKH8vEb4IhT7G2YLYf6_htcMTWx6YGRoOZH9XDKjnQ/viewform?embedded=true"
width="100%" height="800" frameborder="0" marginheight="0" marginwidth="0">Carregando…</iframe>

---

<a name="sobre"></a>
<p align="center">
  <img src="images/caio.jpg" alt="Caio Velasco" width="180" style="border-radius: 50%; margin-top: 10px; margin-bottom: 20px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" />
</p>

**Caio Velasco** é Engenheiro Mecânico _Cum Laude_ pela **UFRJ** e Mestre em Economia e Políticas Públicas pela **UCLA**, com bolsa da Fundação Lemann. Atualmente mora em Barcelona, na Espanha.

Depois de pausar um doutorado em Economia na Holanda durante a pandemia, passou a atuar remotamente em Projetos de Engenharia e Ciência de Dados, ajudando empresas dos EUA, Europa e Brasil a construírem **plataformas modernas** e **preparar modelos** para responder perguntas de negócio. Porém, a paixão por ensinar é o que mais o motiva.

Antes disso, trabalhou com FinTech e EdTech e fundou o **MePrepara**, plataforma online que ensinou matemática para as provas GRE e GMAT.

Reconhecido por instituições como **UCLA, Yale, Fundação Lemann, General Electric Foundation e Club of Rome na Suiça**, Caio combina **engenharia, economia e educação** - e acredita que **entender a matemática por trás dos modelos sem perder a intuição e a aplicação prática** é essencial.

> 💡 “Meu objetivo é ensinar com clareza e propósito - conectando teoria, intuição e aplicação prática.”