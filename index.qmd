---
title: "Do Zero Ã  RegressÃ£o Linear - com Dois Sabores"
format: html
---

<a name="topo"></a>

> Eu nÃ£o quero vender um curso. **Quero vender uma forma de pensar**.

## Minha MotivaÃ§Ã£o <a name="motivacao"></a>

Em um vÃ­deo de um fÃ­sico americano muito famoso, _Richard Feynman_ diz que **toda explicaÃ§Ã£o depende de um quadro de referÃªncia compartilhado entre quem pergunta e quem responde**.  
Antes de responder *por que* algo acontece, Ã© preciso **concordar sobre o que jÃ¡ sabemos ser verdade**.

Ã‰ aÃ­ que mora o perigo. A maioria das pessoas ainda nÃ£o estÃ¡ confortÃ¡vel com (e ainda nÃ£o domina) a formalizaÃ§Ã£o matemÃ¡tica e estatÃ­stica que estÃ¡ atrÃ¡s de muito fenÃ´meno, inclusive no mundo dos **dados**. Portanto, Ã© nesse _gap_ que entra a minha **filosofia de ensino**. Eu quero te convencer que:

- **VocÃª Ã© capaz sim de entender coisas complexas**, mas precisa de alguÃ©m capaz de traduzi-las para o seu quadro de referÃªncia atual
- **VocÃª Ã© capaz sim de expandir seu quadro de referÃªncia atual**, para poder ter espaÃ§o para colocar as verdadeiras respostas das suas possÃ­veis perguntas. 

Ã‰ exatamente assim com a RegressÃ£o Linear â€” e com a CiÃªncia de Dados em geral.

Quando alguÃ©m pergunta *â€œpor que o modelo funciona?â€*, a resposta exige atravessar vÃ¡rias camadas: _Ã¡lgebra linear, probabilidade, inferÃªncia, otimizaÃ§Ã£o_.  

- Se vocÃª nÃ£o conhece essas bases, a explicaÃ§Ã£o soa misteriosa ou com certeza eu estarei trapaceando para te responder;  
- Se vocÃª conhece, ela se torna Ã³bvia, elegante e te empodera no longo prazo.

Este curso nasce dessa motivaÃ§Ã£o:  

- **Criar um caminho que reconstrÃ³i o quadro conceitual** para que a matemÃ¡tica e estatÃ­stica passem a fazer sentido.

> â€œEntender Ã© compartilhar o mesmo vocabulÃ¡rio da explicaÃ§Ã£o e pressupÃµe que conseguimos atingir o mesmo quadro de referÃªncia.â€

## Exemplo

**Aluno**:  
- â€œCaio, por que a RegressÃ£o Linear funciona? Por que a gente usa isso?â€

**Eu (sorrindo)**:  
- â€œDepende do que vocÃª quer dizer com "funciona".
- Se vocÃª quer dizer â€˜_por que a reta se ajusta aos pontos_â€™, entÃ£o estamos falando de **Ã¡lgebra linear** e **geometria**.
- Se vocÃª quer dizer â€˜_por que ela estima a relaÃ§Ã£o verdadeira entre Y e X_â€™, entramos em **probabilidade** e **inferÃªncia**.
- E se vocÃª quer dizer â€˜_por que ela prevÃª bem novos dados_â€™, aÃ­ jÃ¡ estamos em **otimizaÃ§Ã£o** e **machine learning**.

EntÃ£o... a resposta depende do quadro em que vocÃª estÃ¡!

E Ã© exatamente isso que eu quero construir neste curso: te nivelar por cima para vocÃª atingir o quadro conceitual onde tudo isso se conecta.

## ğŸ¯ Objetivo do Curso <a name="objetivo"></a>

Dar uma formaÃ§Ã£o sÃ³lida em **MatemÃ¡tica, EstatÃ­stica e Machine Learning**,  
tendo a **RegressÃ£o Linear** como fio condutor para unir teoria, geometria e prÃ¡tica.

VocÃª vai compreender:

- O *mundo da EstatÃ­stica (inferÃªncia)* - como estimamos, testamos e interpretamos relaÃ§Ãµes entre variÃ¡veis.  
- O *mundo do Machine Learning (prediÃ§Ã£o e otimizaÃ§Ã£o)* - como treinamos modelos para prever e generalizar.  
- E que ambos partem da mesma base: **Ã¡lgebra linear + probabilidade + otimizaÃ§Ã£o**.

## ğŸ¤” Por que este curso Ã© diferente e Por que â€œdois saboresâ€?

A maioria dos cursos ensina regressÃ£o como uma receita.  
Aqui, vocÃª vai **entender o porquÃª** de cada passo - e ver como a matemÃ¡tica, quando bem explicada, se torna **clara, visual e atÃ© bonita**.

- ğŸ‘ï¸ **Visual:** vocÃª vai enxergar o que as equaÃ§Ãµes representam no espaÃ§o.  
- ğŸ”¢ **MatemÃ¡tico:** aprender o essencial de Ã¡lgebra, probabilidade e otimizaÃ§Ã£o no contexto certo.  
- ğŸ’¡ **Aplicado:** ver a teoria funcionando em Python e em dados reais.  
- ğŸ¯ **Conectado:** entender como EstatÃ­stica e Machine Learning sÃ£o duas formas de olhar o mesmo problema.

AlÃ©m disso, vocÃª vai aprender **a mesma RegressÃ£o Linear** sob dois paradigmas:  
- **EstatÃ­stico (inferÃªncia e causalidade)** â€” entender *por que* e *como* as variÃ¡veis se relacionam.  
- **Computacional (prediÃ§Ã£o e otimizaÃ§Ã£o)** â€” treinar modelos que funcionam bem em novos dados.

Algum desses dois modos de pensar aparecem em praticamente todas as indÃºstrias:

- ğŸ¥ **EducaÃ§Ã£o, SaÃºde e Epidemiologia:** regressÃ£o Ã© usada para estimar o efeito de medicamentos, identificar fatores de risco e ajustar por variÃ¡veis de confusÃ£o - foco na **inferÃªncia causal**.  
- ğŸ›’ **E-commerce e Marketing:** usada para prever vendas, calcular propensÃ£o Ã  compra ou estimar o impacto de campanhas - foco na **prediÃ§Ã£o e otimizaÃ§Ã£o**.  
- âš™ï¸ **Engenharia e IoT:** modelos lineares ajudam a prever falhas, calibrar sensores e otimizar desempenho de mÃ¡quinas - combinando **modelagem estatÃ­stica e preditiva**.

Em todos esses contextos, a matemÃ¡tica Ã© a mesma - o que muda Ã© **o propÃ³sito**: explicar ou prever.

## ğŸ§© PrÃ©-requisitos <a name="requisitos"></a>

Este curso foi feito para quem **quer aprender RegressÃ£o Linear de verdade - da base Ã  aplicaÃ§Ã£o prÃ¡tica**,
mesmo sem formaÃ§Ã£o formal em EstatÃ­stica ou Engenharia.

### ğŸ“ Conhecimentos desejÃ¡veis
- **MatemÃ¡tica bÃ¡sica do ensino mÃ©dio:** fraÃ§Ãµes, potÃªncias e equaÃ§Ãµes lineares simples.  
- **FunÃ§Ãµes e grÃ¡ficos:** o que significa uma reta, inclinaÃ§Ã£o, intercepto e plano $x$-$y$.  
- **Leitura em inglÃªs tÃ©cnico:** termos como *dataset*, *model*, *gradient*.  
- **Curiosidade com programaÃ§Ã£o:** Python Ã© bem-vindo, mas nÃ£o obrigatÃ³rio.

## ğŸ“˜ ConteÃºdo Completo <a name="modulos"></a>

O curso foi desenhado para que cada conceito matemÃ¡tico surja como uma resposta natural a uma pergunta prÃ¡tica.  
VocÃª nÃ£o vai decorar fÃ³rmulas: vai **entender por que elas existem**, o que representam e como se conectam.

### Estrutura Geral

| MÃ³dulo | Tema | ÃŠnfase |
|--------|------|--------|
| 0 | O que Ã© modelar dados | IntuiÃ§Ã£o e motivaÃ§Ã£o |
| 1 | Fundamentos MatemÃ¡ticos | Ãlgebra Linear + OtimizaÃ§Ã£o |
| 2 | Fundamentos EstatÃ­sticos | Probabilidade + InferÃªncia |
| 3 | RegressÃ£o Linear (EstatÃ­stica) | OLS e interpretaÃ§Ã£o |
| 4 | RegressÃ£o Linear (ML) | Gradient Descent e prediÃ§Ã£o |

### Ãlgebra Linear: a casa da regressÃ£o linear

-> _Como uma equaÃ§Ã£o vira um modelo? Onde a regressÃ£o realmente â€œviveâ€?_

- Cada coluna do dataset ($X_1, X_2, ..., X_p$) Ã© uma **variÃ¡vel explicativa**, uma direÃ§Ã£o em um espaÃ§o vetorial.
O conjunto dessas colunas forma o **subespaÃ§o de colunas de $X$** - Ã© dentro dele que o modelo tenta â€œreconstruirâ€ $Y$, a variÃ¡vel que queremos explicar.
- **O dataset como matriz**: um conjunto de observaÃ§Ãµes pode ser escrito como uma **matriz $X$ de tamanho $n \times p$**, onde cada linha representa uma observaÃ§Ã£o e cada coluna, uma variÃ¡vel. Ã‰ aqui que o dataset ganha **forma geomÃ©trica**: um objeto que podemos manipular, projetar e decompor.
- **EspaÃ§o e subespaÃ§o**: representam **todas as combinaÃ§Ãµes possÃ­veis das variÃ¡veis**. Mostram atÃ© onde o modelo pode ir - o que ele consegue e o que jamais explicarÃ¡.
- **Base e dimensÃ£o**: indicam **quantas variÃ¡veis realmente trazem informaÃ§Ã£o nova**, e quando algumas apenas repetem o que outras jÃ¡ dizem (**multicolinearidade**).
- **ProjeÃ§Ã£o**: ajustar a regressÃ£o Ã© **projetar $Y$ sobre o espaÃ§o gerado por $X$** - encontrar a melhor combinaÃ§Ã£o linear das colunas de $X$ que aproxima $Y$.
- **Ortogonalidade e resÃ­duos**: os resÃ­duos sÃ£o **perpendiculares** a esse espaÃ§o; isso significa que o modelo **jÃ¡ extraiu tudo o que $X$ podia oferecer**.

- **SoluÃ§Ã£o matricial**: $\hat{\beta} = (X'X)^{-1}X'Y$ expressa exatamente o processo de projeÃ§Ã£o de $Y$ sobre o espaÃ§o das colunas de $X$. A fÃ³rmula mostra como o modelo â€œcombinaâ€ as colunas de $X$ para construir a versÃ£o de $Y$ mais prÃ³xima possÃ­vel - $\hat{Y} = X\hat{\beta}$.

> A regressÃ£o linear Ã©, geometricamente, **a melhor projeÃ§Ã£o possÃ­vel** de $Y$ no espaÃ§o definido por $X$.

---

### Probabilidade ClÃ¡ssica - a casa da incerteza presente nos dados

> Antes, $Y$ e $X$ eram vetores no espaÃ§o. Agora, eles passam a ser variÃ¡veis aleatÃ³rias (_random variables_) - porque agora a necessidade Ã© entender o que Ã© incerteza, amostra, etc.

A partir daqui, tratamos o modelo como uma crenÃ§a formal sobre como os dados sÃ£o gerados: um _Data Generating Process (DGP)_ ou um "mecanismo gerador dos dados". Ã‰ o elo entre a realidade e o modelo: o que assumimos ser verdadeiro sobre o processo que produz as observaÃ§Ãµes, ou seja, os dados.

- **O DGP como mecanismo:**  
  Supomos que existe uma relaÃ§Ã£o verdadeira na populaÃ§Ã£o que explica como X se relaciona com Y, e que nossa amostra Ã© apenas uma manifestaÃ§Ã£o aleatÃ³ria desse processo. Ã‰ aqui que nasce a noÃ§Ã£o de incerteza: e de que â€œmodeloâ€ significa *assumir algo sobre o mundo.*

- **VariÃ¡veis AleatÃ³rias (random variables):**  
  As colunas de $X$ e o vetor $Y$ na Ãlgebra Linear agora sÃ£o **variÃ¡veis aleatÃ³rias**, ou seja, objetos que assumem valores diferentes a cada extraÃ§Ã£o do DGP, como uma certa probabilidade atribuÃ­da a cada valor. Tipo quando jogamos uma moeda e cada valor tem chance 50% de aparecer. Esse conceito transforma o dataset $(Y, X)$ abstrato em uma **realizaÃ§Ã£o** de algo maior: uma **amostra aleatÃ³ria**.

- **Tipos de variÃ¡veis:**  
  O tipo de variÃ¡vel define o **espaÃ§o onde ela vive** - um conceito que vem da MatemÃ¡tica mais refinada:

  - Quando o conjunto de valores possÃ­veis Ã© **contÃ¡vel** - finito (como ${0,1}$) ou infinito mas enumerÃ¡vel (como os naturais ${1,2,3,...}$) - dizemos que a variÃ¡vel Ã© **discreta**.  
    Exemplo: nÃºmero de filhos, quantidade de produtos vendidos.  
  - Quando o conjunto de valores Ã© **infinito e nÃ£o enumerÃ¡vel**, dizemos que a variÃ¡vel Ã© **contÃ­nua**. Exemplo: altura, renda, temperatura.

  Essa diferenÃ§a muda completamente a forma da distribuiÃ§Ã£o - e, portanto, do modelo:  
  - Se $Y$ Ã© contÃ­nua â†’ usamos **RegressÃ£o Linear**.  
  - Se $Y$ Ã© binÃ¡ria (0/1) â†’ **RegressÃ£o LogÃ­stica**.  
  - Se alguma variÃ¡vel $X$ Ã© categÃ³rica â†’ representamos por **dummies** (1 para presenÃ§a, 0 para ausÃªncia), criando novas dimensÃµes no espaÃ§o vetorial.

  ğŸ’¡ *Nota:*  
  Uma outra confusÃ£o vem do fato de que **nem toda variÃ¡vel discreta Ã© â€œcategÃ³ricaâ€**. Uma variÃ¡vel pode ser discreta **com ou sem ordem** entre os valores:  
  - **Ordinal** (hÃ¡ ordem, como nÃ­vel de satisfaÃ§Ã£o: â€œbaixoâ€, â€œmÃ©dioâ€, â€œaltoâ€);  
  - **Nominal** (sem ordem, como â€œsexoâ€ ou â€œcor dos olhosâ€).  

Entender a natureza da variÃ¡vel evita erros conceituais - e garante que cada tipo seja modelado de forma coerente com sua estrutura matemÃ¡tica.

- **DistribuiÃ§Ã£o e incerteza:**  
  Cada variÃ¡vel aleatÃ³ria tem uma **distribuiÃ§Ã£o**, que descreve a probabilidade de cada valor possÃ­vel. A distribuiÃ§Ã£o Ã© a forma matemÃ¡tica de expressar a incerteza e o alicerce de todo raciocÃ­nio estatÃ­stico.

- **Amostra aleatÃ³ria:**  
  As observaÃ§Ãµes que temos sÃ£o uma **amostra independente do DGP**. Isso legitima usar os dados observados para aprender sobre a populaÃ§Ã£o.

- **Modelo probabilÃ­stico:**  
  Especificamos a crenÃ§a de que:  
  $$Y_i = \beta_0 + \beta_1 X_{i1} + \cdots + \beta_p X_{ip} + \varepsilon_i,$$  
  onde $\varepsilon_i$ representa o componente aleatÃ³rio, com $\mathbb{E}[\varepsilon_i] = 0$ e $\text{Var}(\varepsilon_i) = \sigma^2$.  
 
 O modelo agora Ã© mais do que uma reta - Ã© uma **afirmaÃ§Ã£o probabilÃ­stica** sobre o comportamento de $Y$ dado $X$.

- **Modelo estatÃ­stico:**  
  Ã‰ o conjunto de todos os possÃ­veis DGPs compatÃ­veis com as suposiÃ§Ãµes (linearidade, homocedasticidade, independÃªncia, normalidade dos erros). Isso define **os limites do que podemos inferir**. Quando essas suposiÃ§Ãµes quebram, o modelo deixa de representar o mundo que imaginamos.

---

> ğŸ¯ A probabilidade dÃ¡ Ã  regressÃ£o um novo significado:  
> nÃ£o mais â€œqual reta cabe melhor nos pontosâ€, mas  
> â€œqual mecanismo plausÃ­vel pode ter gerado esses pontosâ€.

### InferÃªncia EstatÃ­stica

- OLS como **melhor estimador linear nÃ£o viesado (Gaussâ€“Markov)**  
- Erro padrÃ£o, intervalo de confianÃ§a e teste t  
- Teste F e interpretaÃ§Ã£o global do modelo  
- HipÃ³teses e p-valores  
- DiagnÃ³stico de resÃ­duos e suposiÃ§Ãµes do modelo  
- Ideia de Causalidade vs. CorrelaÃ§Ã£o - quando o $\beta$ Ã© causal?

### OtimizaÃ§Ã£o

- RevisÃ£o de derivadas e gradiente vetorial  
- FunÃ§Ãµes convexas e mÃ­nimos locais  
- Derivada matricial da soma dos quadrados  
- Gradient Descent: passo de atualizaÃ§Ã£o e intuiÃ§Ã£o geomÃ©trica  
- FunÃ§Ã£o de perda (Loss Function) e erro empÃ­rico  
- IntroduÃ§Ã£o Ã  regularizaÃ§Ã£o: Ridge e LASSO  
- ConvergÃªncia, taxa de aprendizado e visualizaÃ§Ã£o do gradiente

## ğŸ§± Formato

- **VÃ­deos curtos e progressivos**, lanÃ§ados semanalmente  
- **ExercÃ­cios prÃ¡ticos em Python**  
- **VisualizaÃ§Ãµes geomÃ©tricas e intuiÃ§Ãµes claras**  
- **Comunidade e feedback**

## ğŸ“ Inscreva-se <a name="formulario"></a>

Quer ser avisado quando abrirem as inscriÃ§Ãµes?

<iframe src="https://docs.google.com/forms/d/e/1FAIpQLScGXyserKH8vEb4IhT7G2YLYf6_htcMTWx6YGRoOZH9XDKjnQ/viewform?embedded=true"
width="100%" height="800" frameborder="0" marginheight="0" marginwidth="0">Carregandoâ€¦</iframe>

---

<a name="sobre"></a>
<p align="center">
  <img src="images/caio.jpg" alt="Caio Velasco" width="180" style="border-radius: 50%; margin-top: 10px; margin-bottom: 20px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" />
</p>

**Caio Velasco** Ã© Engenheiro MecÃ¢nico _Cum Laude_ pela **UFRJ** e Mestre em Economia e PolÃ­ticas PÃºblicas pela **UCLA**, com bolsa da FundaÃ§Ã£o Lemann. Atualmente mora em Barcelona, na Espanha.

Depois de pausar um doutorado em Economia na Holanda durante a pandemia, passou a atuar remotamente em Projetos de Engenharia e CiÃªncia de Dados, ajudando empresas dos EUA, Europa e Brasil a construÃ­rem **plataformas modernas** e **preparar modelos** para responder perguntas de negÃ³cio. PorÃ©m, a paixÃ£o por ensinar Ã© o que mais o motiva.

Antes disso, trabalhou com FinTech e EdTech e fundou o **MePrepara**, plataforma online que ensinou matemÃ¡tica para as provas GRE e GMAT.

Reconhecido por instituiÃ§Ãµes como **UCLA, Yale, FundaÃ§Ã£o Lemann, General Electric Foundation e Club of Rome na SuiÃ§a**, Caio combina **engenharia, economia e educaÃ§Ã£o** - e acredita que **entender a matemÃ¡tica por trÃ¡s dos modelos sem perder a intuiÃ§Ã£o e a aplicaÃ§Ã£o prÃ¡tica** Ã© essencial.

> ğŸ’¡ â€œMeu objetivo Ã© ensinar com clareza e propÃ³sito - conectando teoria, intuiÃ§Ã£o e aplicaÃ§Ã£o prÃ¡tica.â€